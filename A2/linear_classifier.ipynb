{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTOX0d3cmLQf"
   },
   "source": "# 2-1: Linear Classifiers"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrfeHl_-m4V-"
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1688739193246,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "WcrhTOZW243H",
    "outputId": "0cb6607a-6c9f-4fd0-c2c8-638f254e2b95",
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:11.613074Z",
     "start_time": "2024-11-24T04:50:11.605200Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "print(ROOT_PATH)\n",
    "PROJECT_PATH = os.path.join(ROOT_PATH)\n",
    "print(os.listdir(PROJECT_PATH))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/y5ryan/private/Deep_Learning_for_CV/A2\n",
      "['two_layer_net.py', 'linear_classifier.ipynb', 'two_layer_net.ipynb', 'cifar-10-python.tar.gz', 'cifar-10-batches-py', 'challenge_problem.ipynb', 'eecs598', 'nn_best_model.pt', '__pycache__', 'linear_classifier.py', 'svm_best_model.pt', 'softmax_best_model.pt']\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7119,
     "status": "ok",
     "timestamp": 1688739204571,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "AhGQF5sw3Fas",
    "outputId": "64d52a39-16cf-4e6a-f4bc-9c622964bff6",
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:11.806906Z",
     "start_time": "2024-11-24T04:50:11.796987Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_PATH)\n",
    "\n",
    "import time, os\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from linear_classifier import hello_linear_classifier\n",
    "hello_linear_classifier()\n",
    "\n",
    "linear_classifier_path = os.path.join(PROJECT_PATH, 'linear_classifier.py')\n",
    "linear_classifier_edit_time = time.ctime(os.path.getmtime(linear_classifier_path))\n",
    "print('linear_classifier.py last edited on %s' % linear_classifier_edit_time)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from linear_classifier.py!\n",
      "linear_classifier.py last edited on Wed Jul  5 20:39:52 2023\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynKS05gJ4iBo"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN1SShPR4lJV"
   },
   "source": [
    "## Setup code\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VUCKw4Tl1ddj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688739211595,
     "user_tz": -540,
     "elapsed": 4518,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:11.827488Z",
     "start_time": "2024-11-24T04:50:11.819194Z"
    }
   },
   "source": [
    "import eecs598\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhqpd2IN2O-K"
   },
   "source": [
    "Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688739220003,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "SGDxdBIMRX6b",
    "outputId": "dfe6f8f1-66b7-4f55-d067-c5b74a76f329",
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:11.839241Z",
     "start_time": "2024-11-24T04:50:11.829881Z"
    }
   },
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yv3zQYw5B3s"
   },
   "source": [
    "## Load the CIFAR-10 dataset\n",
    "Then, we will first load the CIFAR-10 dataset, same as knn. The utility function `eecs598.data.preprocess_cifar10()` returns the entire CIFAR-10 dataset as a set of six **Torch tensors**:\n",
    "\n",
    "- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n",
    "- `y_train` contains all training labels (integers in the range $[0, 9]$)\n",
    "- `X_val` contains all validation images\n",
    "- `y_val` contains all validation labels\n",
    "- `X_test` contains all test images\n",
    "- `y_test` contains all test labels\n",
    "\n",
    "In this notebook we will use the **bias trick**: By adding an extra constant feature of ones to each image, we avoid the need to keep track of a bias vector; the bias will be encoded as the part of the weight matrix that interacts with the constant ones in the input.\n",
    "\n",
    "In the `two_layer_net.ipynb` notebook that follows this one, we will not use the bias trick.\n",
    "\n",
    "We can learn more about the `eecs598.data.preprocess_cifar10` function by invoking the `help` command:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688615265830,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "V2mFlFmQ1ddm",
    "outputId": "354831b6-5930-42df-dcb2-c56e9e2a8f27",
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:11.851507Z",
     "start_time": "2024-11-24T04:50:11.840856Z"
    }
   },
   "source": [
    "import eecs598\n",
    "help(eecs598.data.preprocess_cifar10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function preprocess_cifar10 in module eecs598.data:\n",
      "\n",
      "preprocess_cifar10(cuda=True, show_examples=True, bias_trick=False, flatten=True, validation_ratio=0.2, dtype=torch.float32)\n",
      "    Returns a preprocessed version of the CIFAR10 dataset, automatically\n",
      "    downloading if necessary. We perform the following steps:\n",
      "    \n",
      "    (0) [Optional] Visualize some images from the dataset\n",
      "    (1) Normalize the data by subtracting the mean\n",
      "    (2) Reshape each image of shape (3, 32, 32) into a vector of shape (3072,)\n",
      "    (3) [Optional] Bias trick: add an extra dimension of ones to the data\n",
      "    (4) Carve out a validation set from the training set\n",
      "    \n",
      "    Inputs:\n",
      "    - cuda: If true, move the entire dataset to the GPU\n",
      "    - validation_ratio: Float in the range (0, 1) giving the fraction of the train\n",
      "      set to reserve for validation\n",
      "    - bias_trick: Boolean telling whether or not to apply the bias trick\n",
      "    - show_examples: Boolean telling whether or not to visualize data samples\n",
      "    - dtype: Optional, data type of the input image X\n",
      "    \n",
      "    Returns a dictionary with the following keys:\n",
      "    - 'X_train': `dtype` tensor of shape (N_train, D) giving training images\n",
      "    - 'X_val': `dtype` tensor of shape (N_val, D) giving val images\n",
      "    - 'X_test': `dtype` tensor of shape (N_test, D) giving test images\n",
      "    - 'y_train': int64 tensor of shape (N_train,) giving training labels\n",
      "    - 'y_val': int64 tensor of shape (N_val,) giving val labels\n",
      "    - 'y_test': int64 tensor of shape (N_test,) giving test labels\n",
      "    \n",
      "    N_train, N_val, and N_test are the number of examples in the train, val, and\n",
      "    test sets respectively. The precise values of N_train and N_val are determined\n",
      "    by the input parameter validation_ratio. D is the dimension of the image data;\n",
      "    if bias_trick is False, then D = 32 * 32 * 3 = 3072;\n",
      "    if bias_trick is True then D = 1 + 32 * 32 * 3 = 3073.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COCx2kM6XB3K"
   },
   "source": [
    "We can now run the `eecs598.data.preprocess` function to get our data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "executionInfo": {
     "elapsed": 24066,
     "status": "ok",
     "timestamp": 1688645918160,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "r_BhZ_6_XB3K",
    "outputId": "058ff151-ed3e-41c1-e415-69648dca6c4d",
    "ExecuteTime": {
     "end_time": "2024-11-24T04:50:13.275715Z",
     "start_time": "2024-11-24T04:50:11.874218Z"
    }
   },
   "source": [
    "# Invoke the above function to get our data.\n",
    "import eecs598\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "data_dict = eecs598.data.preprocess_cifar10(bias_trick=True, cuda=True, dtype=torch.float64)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ],
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 23.69 GiB total capacity; 1.14 GiB already allocated; 141.69 MiB free; 1.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_13003/2372636187.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0meecs598\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mdata_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meecs598\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocess_cifar10\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_trick\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcuda\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Train data shape: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'X_train'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Train labels shape: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'y_train'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/private/Deep_Learning_for_CV/A2/eecs598/data.py\u001B[0m in \u001B[0;36mpreprocess_cifar10\u001B[0;34m(cuda, show_examples, bias_trick, flatten, validation_ratio, dtype)\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mX_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0my_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m         \u001B[0mX_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX_test\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0my_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_test\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 23.69 GiB total capacity; 1.14 GiB already allocated; 141.69 MiB free; 1.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lvdm4fm7iJC"
   },
   "source": [
    "# SVM Classifier\n",
    "\n",
    "In this section, you will:\n",
    "    \n",
    "- implement a fully-vectorized **loss function** for the SVM\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** using numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n",
    "\n",
    "In Assignment 2, you SHOULD NOT use \".to()\" or \".cuda()\" in each implementation block. Otherwise, your implementation would gives you an error in Autograder end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTI4qN7S9aTr"
   },
   "source": [
    "First, we will test the naive version of svm loss in `linear_classifier.py`. Let's first try the naive implementation of the loss we provided for you. You will get 9.000197. (Note: we've provided the `loss` part of the `svm_loss_naive` function, so you don't need to re-implement in `svm_loss_naive`. However, if your loss value doesn't match, then please report this to [Piazza](https://piazza.com/class/kxtai72amx34p0))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18473,
     "status": "ok",
     "timestamp": 1688615391462,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     },
     "user_tz": -540
    },
    "id": "Hxzu2uZq9P8P",
    "outputId": "23122d1e-a296-4bf1-a515-9574fb01abbb"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import svm_loss_naive\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "# generate a random SVM weight tensor of small numbers\n",
    "W = torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device) * 0.0001\n",
    "\n",
    "loss, _grad_ = svm_loss_naive(W, data_dict['X_val'], data_dict['y_val'], 0.000005)\n",
    "print('loss: %f' % (loss, ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LbRTXJ39Yp8"
   },
   "source": [
    "The `_grad_` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `svm_loss_naive`, by filing out the TODO blocks. You will find it helpful to interweave your new code inside the existing function.\n",
    "\n",
    "To check that you have implemented the gradient correctly, we will use **numeric gradient checking**: we will use a finite differences approach to numerically estimate the gradient of the forward pass, and compare this numeric gradient to the analytic gradient that you implemented.\n",
    "\n",
    "We have provided a function `eecs598.grad.grad_check_sparse` to help with numeric gradient checking. You can learn more about this function using the `help` command:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1688540776960,
     "user": {
      "displayName": "정화식",
      "userId": "11198258190229176580"
     },
     "user_tz": -540
    },
    "id": "wzie6dahXB3Q",
    "outputId": "8e26e355-2d79-4807-88cf-ed5c77538a8d"
   },
   "source": [
    "import eecs598\n",
    "help(eecs598.grad.grad_check_sparse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKc4v6a8XB3T"
   },
   "source": [
    "Now run the following to perform numeric gradient checking on the gradients of your SVM loss. You should see relative errors less than `1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3126,
     "status": "ok",
     "timestamp": 1688547480581,
     "user": {
      "displayName": "정화식",
      "userId": "11198258190229176580"
     },
     "user_tz": -540
    },
    "id": "o3sMha4i9p_V",
    "outputId": "4ae76c56-1e32-47f6-c790-5b4dac93ea12"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import svm_loss_naive\n",
    "\n",
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Use a random W and a minibatch of data from the val set for gradient checking\n",
    "# For numeric gradient checking it is a good idea to use 64-bit floating point\n",
    "# numbers for increased numeric precision; however when actually training models\n",
    "# we usually use 32-bit floating point numbers for increased speed.\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "batch_size = 64\n",
    "X_batch = data_dict['X_val'][:batch_size]\n",
    "y_batch = data_dict['y_val'][:batch_size]\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "# YOUR_TURN: implement the gradient part of 'svm_loss_naive' function in \"linear_classifier.py\"\n",
    "_, grad = svm_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should\n",
    "# match almost exactly along all dimensions.\n",
    "f = lambda w: svm_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n",
    "grad_numerical = eecs598.grad.grad_check_sparse(f, W, grad)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSdsG-L292Ww"
   },
   "source": [
    "Let's do the gradient check once again with regularization turned on. (You didn't forget the regularization gradient, did you?)\n",
    "\n",
    "You should see relative errors less than `1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2445,
     "status": "ok",
     "timestamp": 1688540807088,
     "user": {
      "displayName": "정화식",
      "userId": "11198258190229176580"
     },
     "user_tz": -540
    },
    "id": "bH6lXxVn9xZk",
    "outputId": "d459a24c-155b-40ed-f1c7-78c628c37758"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import svm_loss_naive\n",
    "\n",
    "# Use a minibatch of data from the val set for gradient checking\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "batch_size = 64\n",
    "X_batch = data_dict['X_val'][:batch_size]\n",
    "y_batch = data_dict['y_val'][:batch_size]\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "# YOUR_TURN: check your 'svm_loss_naive' implementation with different 'reg'\n",
    "_, grad = svm_loss_naive(W, X_batch, y_batch, reg=1e3)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should\n",
    "# match almost exactly along all dimensions.\n",
    "f = lambda w: svm_loss_naive(w, X_batch, y_batch, reg=1e3)[0]\n",
    "grad_numerical = eecs598.grad.grad_check_sparse(f, W, grad)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc5Wtu-e-WlI"
   },
   "source": [
    "Now, let's implement vectorized version of SVM: `svm_loss_vectorized`. It should compute the same inputs and outputs as the naive version before, but it should involve **no explicit loops**.\n",
    "\n",
    "Let's first check the speed and performance bewteen the non-vectorized and the vectorized version. You should see a 15-120x speedup. PyTorch does some extra setup the first time you run CUDA code, so **you may need to run this cell more than once to see the desired speedup**.\n",
    "\n",
    "(Note: It may have some difference, but should be less than 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pBLqLTAGo1Rs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688615491328,
     "user_tz": -540,
     "elapsed": 532,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "8a9782ff-4d23-432f-e7b4-9d5b2705c335"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import svm_loss_naive, svm_loss_vectorized\n",
    "\n",
    "# Use random weights and a minibatch of val data for gradient checking\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "reg = 0.000005\n",
    "\n",
    "# Run and time the naive version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('Naive loss: %e computed in %.2fms' % (loss_naive, ms_naive))\n",
    "\n",
    "# Run and time the vectorized version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "# YOUR_TURN: implement the loss part of 'svm_loss_vectorized' function in \"linear_classifier.py\"\n",
    "loss_vec, _ = svm_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('Vectorized loss: %e computed in %.2fms' % (loss_vec, ms_vec))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('Difference: %.2e' % (loss_naive - loss_vec))\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRDPpAMl-0WD"
   },
   "source": [
    "Then, let's compute the gradient of the loss function. We can check the difference of gradient as well. (The error should be less than 1e-6)\n",
    "\n",
    "Now implement a vectorized version of the gradient computation in `svm_loss_vectorize` above. Run the cell below to compare the gradient of your naive and vectorized implementations. The difference between the gradients should be less than `1e-6`, and the vectorized version should run 15-120x faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3_SyWrTJ-OfX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688615538161,
     "user_tz": -540,
     "elapsed": 511,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "edc06642-c390-41b8-cc7c-ef1885b40775"
   },
   "source": [
    "# The naive implementation and the vectorized implementation should match, but\n",
    "# the vectorized version should still be much faster.\n",
    "\n",
    "import eecs598\n",
    "from linear_classifier import svm_loss_naive, svm_loss_vectorized, sample_batch\n",
    "\n",
    "# Use random weights and a minibatch of val data for gradient checking\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "reg = 0.000005\n",
    "\n",
    "# Run and time the naive version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_batch, y_batch, 0.000005)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('Naive loss and gradient: computed in %.2fms' % ms_naive)\n",
    "\n",
    "# Run and time the vectorized version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "# YOUR_TURN: implement the gradient part of 'svm_loss_vectorized' function in \"linear_classifier.py\"\n",
    "_, grad_vec = svm_loss_vectorized(W, X_batch, y_batch, 0.000005)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('Vectorized loss and gradient: computed in %.2fms' % ms_vec)\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a tensor, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "grad_difference = torch.norm(grad_naive - grad_vec, p='fro')\n",
    "print('Gradient difference: %.2e' % grad_difference)\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU852IitCtrC"
   },
   "source": [
    "Now that we have an efficient vectorized implementation of the SVM loss and its gradient, we can implement a training pipeline for linear classifiers.\n",
    "\n",
    "Please complete the implementation of `train_linear_classifier` in `linear_classifer.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6IL1_D9wCbF"
   },
   "source": [
    "Once you have implemented the training function, run the following cell to train a linear classifier using some default hyperparameters:\n",
    "\n",
    "(You should see a final loss close to 9.0, and your training loop should run in about two seconds)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QaEZkCe3-kOu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646442807,
     "user_tz": -540,
     "elapsed": 2381,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "1f59d7d6-88be-4284-a15e-e6283fd1d6fa"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import svm_loss_vectorized, train_linear_classifier\n",
    "\n",
    "# fix random seed before we perform this operation\n",
    "eecs598.reset_seed(0)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "\n",
    "# YOUR_TURN: Implement how to construct the batch,\n",
    "#            and how to update the weight in 'train_linear_classifier'\n",
    "W, loss_hist = train_linear_classifier(svm_loss_vectorized, None,\n",
    "                                       data_dict['X_train'],\n",
    "                                       data_dict['y_train'],\n",
    "                                       learning_rate=3e-11, reg=2.5e4,\n",
    "                                       num_iters=1500, verbose=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8mz9aXfDrsF"
   },
   "source": [
    "A useful debugging strategy is to plot the loss as a function of iteration number. In this case it seems our hyperparameters are not good, since the training loss is not decreasing very fast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JJ8GjaZS_MLe",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688622651635,
     "user_tz": -540,
     "elapsed": 683,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "ed8c0cdb-9060-4f52-be5c-fc2b65bd5e21"
   },
   "source": [
    "plt.plot(loss_hist, 'o')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRdfknKsE6F2"
   },
   "source": [
    "Then, let's move on to the prediction stage. We can evaluate the performance our trained model on both the training and validation set. You should see validation accuracy less than 20%."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YfToPzce_OBH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688624660379,
     "user_tz": -540,
     "elapsed": 314,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "5389e3de-98a3-4a89-e7fe-cd18f5b6bcbd"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import predict_linear_classifier\n",
    "\n",
    "# fix random seed before we perform this operation\n",
    "eecs598.reset_seed(0)\n",
    "\n",
    "# evaluate the performance on both the training and validation set\n",
    "# YOUR_TURN: Implement how to make a prediction with the trained weight\n",
    "#            in 'predict_linear_classifier'\n",
    "y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n",
    "train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).double().mean().item()\n",
    "print('Training accuracy: %.2f%%' % train_acc)\n",
    "\n",
    "y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n",
    "val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWIyGnMOFOV8"
   },
   "source": [
    "Unfortunately, the performance of our initial model is quite bad. To find a better hyperparamters, we first modulized the functions that we've implemented as LinearSVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taNmjt2wGJQr"
   },
   "source": [
    "Now, please use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n",
    "\n",
    "To get full credit for the assignment your best model found through cross-validation should achieve an accuracy of at least 37% on the validation set.\n",
    "\n",
    "(Our best model got over 38.1% -- did you beat us?)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oVMsJ9Ti_Ude",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646004666,
     "user_tz": -540,
     "elapsed": 76211,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "ccc8dcc9-de18-4ab6-adc5-fd75d68d05e7"
   },
   "source": [
    "import os\n",
    "import eecs598\n",
    "from linear_classifier import LinearSVM, svm_get_search_params, test_one_param_set\n",
    "\n",
    "# YOUR_TURN: find the best learning_rates and regularization_strengths combination\n",
    "#            in 'svm_get_search_params'\n",
    "learning_rates, regularization_strengths = svm_get_search_params()\n",
    "num_models = len(learning_rates) * len(regularization_strengths)\n",
    "\n",
    "####\n",
    "# It is okay to comment out the following conditions when you are working on svm_get_search_params.\n",
    "# But, please do not forget to reset back to the original setting once you are done.\n",
    "if num_models > 25:\n",
    "  raise Exception(\"Please do not test/submit more than 25 items at once\")\n",
    "elif num_models < 5:\n",
    "  raise Exception(\"Please present at least 5 parameter sets in your final ipynb\")\n",
    "####\n",
    "\n",
    "\n",
    "i = 0\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (train_acc, val_acc).\n",
    "results = {}\n",
    "best_val = -1.0   # The highest validation accuracy that we have seen so far.\n",
    "best_svm_model = None # The LinearSVM object that achieved the highest validation rate.\n",
    "num_iters = 2000 # number of iterations\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for reg in regularization_strengths:\n",
    "    i += 1\n",
    "    print('Training SVM %d / %d with learning_rate=%e and reg=%e'\n",
    "          % (i, num_models, lr, reg))\n",
    "\n",
    "    eecs598.reset_seed(0)\n",
    "    # YOUR_TURN: implement a function that gives the trained model with\n",
    "    #            train/validation accuracies in 'test_one_param_set'\n",
    "    #            (note: this function will be used in Softmax Classifier section as well)\n",
    "    cand_svm_model, cand_train_acc, cand_val_acc = test_one_param_set(LinearSVM(), data_dict, lr, reg, num_iters)\n",
    "\n",
    "    if cand_val_acc > best_val:\n",
    "      best_val = cand_val_acc\n",
    "      best_svm_model = cand_svm_model # save the svm\n",
    "    results[(lr, reg)] = (cand_train_acc, cand_val_acc)\n",
    "\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "  train_acc, val_acc = results[(lr, reg)]\n",
    "  print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "         lr, reg, train_acc, val_acc))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "# save the best model\n",
    "path = os.path.join(GOOGLE_DRIVE_PATH, 'svm_best_model.pt')\n",
    "best_svm_model.save(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBbvJvMeGZ-7"
   },
   "source": [
    "Visualize the cross-validation results. You can use this as a debugging tool -- after examining the cross-validation results here, you may want to go back and rerun your cross-validation from above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QbPffK9H_ZGj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688624977039,
     "user_tz": -540,
     "elapsed": 1066,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "999b1e88-1d67-4ac8-e86e-65755fe5c10d"
   },
   "source": [
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCMVzxquGf1O"
   },
   "source": [
    "Evaluate the best svm on test set. To get full credit for the assignment you should achieve a test-set accuracy above 35%.\n",
    "\n",
    "(Our best was over 39.1% -- did you beat us?)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "maJ7use3_soL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646029396,
     "user_tz": -540,
     "elapsed": 581,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "7bb2e91d-7500-4963-fb28-11a8bada6d6d"
   },
   "source": [
    "import eecs598\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "y_test_pred = best_svm_model.predict(data_dict['X_test'])\n",
    "test_accuracy = torch.mean((data_dict['y_test'] == y_test_pred).double()) * 100\n",
    "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-QVIG4fGiqJ"
   },
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "McLHYtFd_vSI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646039939,
     "user_tz": -540,
     "elapsed": 1529,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "f37998fb-1df5-48bb-c2e9-4af34d318e87"
   },
   "source": [
    "w = best_svm_model.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(3, 32, 32, 10)\n",
    "w = w.transpose(0, 2).transpose(1, 0)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkuwyMY27RxS"
   },
   "source": [
    "# Softmax Classifier\n",
    "\n",
    "Similar to the SVM, you will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n",
    "\n",
    "As noted in the SVM section, you SHOULD NOT use \".to()\" or \".cuda()\" in each implementation block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLJMVGtvIgo3"
   },
   "source": [
    "First, let's start from implementing the naive softmax loss function with nested loops in `softmax_loss_naive` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cER8fiSq7Ys-"
   },
   "source": [
    "As a sanity check to see whether we have implemented the loss correctly, run the softmax classifier with a small random weight matrix and no regularization. You should see loss near log(10) = 2.3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V9q77O7F7VI6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688626550733,
     "user_tz": -540,
     "elapsed": 1069,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "a3ac2efe-1f1a-4741-cfd5-f94990d80b83"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "# Generate a random softmax weight tensor and use it to compute the loss.\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# YOUR_TURN: Complete the implementation of softmax_loss_naive and implement\n",
    "# a (naive) version of the gradient that uses nested loops.\n",
    "loss, _ = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to log(10.0).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (math.log(10.0)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QJzUHl5I0HH"
   },
   "source": [
    "Next, we use gradient checking to debug the analytic gradient of our naive softmax loss function. If you've implemented the gradient correctly, you should see relative errors less than `1e-5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lj6YpN3q1hVG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688626667583,
     "user_tz": -540,
     "elapsed": 3444,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "eb930829-a4b1-4861-9abb-7e899241c443"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# YOUR_TURN: Complete the implementation of softmax_loss_naive and implement\n",
    "# a (naive) version of the gradient that uses nested loops.\n",
    "_, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n",
    "eecs598.grad.grad_check_sparse(f, W, grad, 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFcgeajBI-L3"
   },
   "source": [
    "Let's perform another gradient check with regularization enabled. Again you should see relative errors less than `1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ik0i21sszZzg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688626688216,
     "user_tz": -540,
     "elapsed": 3291,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "ebffc387-003c-47ca-dd1f-4559a7cd9845"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "eecs598.reset_seed(128)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "reg = 10.0\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# YOUR_TURN: Complete the gradient compuation part of softmax_loss_naive\n",
    "_, grad = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg)[0]\n",
    "eecs598.grad.grad_check_sparse(f, W, grad, 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQgRzrdRJAm7"
   },
   "source": [
    "Then, let's move on to the vectorized form: `softmax_loss_vectorized`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88xZ0rbLJGKV"
   },
   "source": [
    "Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in softmax_loss_vectorized. The two versions should compute the same results, but the vectorized version should be much faster.\n",
    "\n",
    "The differences between the naive and vectorized losses and gradients should both be less than `1e-6`, and your vectorized implementation should be at least 20x faster than the naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lGNAe-oP1dds",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688627774798,
     "user_tz": -540,
     "elapsed": 1021,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "6e58fcbe-5482-43cd-d8ea-fbb3bc78f458"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "reg = 0.05\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Run and time the naive version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, ms_naive))\n",
    "\n",
    "# Run and time the vectorized version\n",
    "# YOUR_TURN: Complete the implementation of softmax_loss_vectorized\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vec, ms_vec))\n",
    "\n",
    "# we use the Frobenius norm to compare the two versions of the gradient.\n",
    "loss_diff = (loss_naive - loss_vec).abs().item()\n",
    "grad_diff = torch.norm(grad_naive - grad_vec, p='fro')\n",
    "print('Loss difference: %.2e' % loss_diff)\n",
    "print('Gradient difference: %.2e' % grad_diff)\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqZScXKyq6WB"
   },
   "source": [
    "Let's check that your implementation of the softmax loss is numerically stable.\n",
    "\n",
    "If either of the following print `nan` then you should double-check the numeric stability of your implementations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bCyFPWxxq58R",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646641148,
     "user_tz": -540,
     "elapsed": 589,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "4f4bf98b-b987-4372-b2fb-67988db169b6"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "device = data_dict['X_train'].device\n",
    "dtype = data_dict['X_train'].dtype\n",
    "D = data_dict['X_train'].shape[1]\n",
    "C = 10\n",
    "\n",
    "# YOUR_TURN: train_linear_classifier should be same as what you've implemented in the SVM section\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_naive, W_ones,\n",
    "                                       data_dict['X_train'],\n",
    "                                       data_dict['y_train'],\n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)\n",
    "\n",
    "\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, W_ones,\n",
    "                                       data_dict['X_train'],\n",
    "                                       data_dict['y_train'],\n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR4JGKoek8FB"
   },
   "source": [
    "Now lets train a softmax classifier with some default hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kqga1rvjk7b8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688628335644,
     "user_tz": -540,
     "elapsed": 4059,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "82991977-523f-487f-e65c-4c4e1083446f"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import softmax_loss_vectorized\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "\n",
    "# YOUR_TURN: train_linear_classifier should be same as what you've implemented in the SVM section\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, None,\n",
    "                                       data_dict['X_train'],\n",
    "                                       data_dict['y_train'],\n",
    "                                       learning_rate=1e-10, reg=2.5e4,\n",
    "                                       num_iters=1500, verbose=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKjxCGwkorCc"
   },
   "source": [
    "Plot the loss curve:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K29x-DWNoujL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688628343707,
     "user_tz": -540,
     "elapsed": 546,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "3667e168-f290-4e99-e440-326b30d148ab"
   },
   "source": [
    "plt.plot(loss_hist, 'o')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WvpBuJWSwfd"
   },
   "source": [
    "Let's compute the accuracy of current model. It should be less than 10%."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zb8kY2MjSvfH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688646719506,
     "user_tz": -540,
     "elapsed": 538,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "b968c72c-9a1b-4148-8ed2-d1c59a0b4e59"
   },
   "source": [
    "import eecs598\n",
    "from linear_classifier import predict_linear_classifier\n",
    "\n",
    "eecs598.reset_seed(0)\n",
    "\n",
    "# evaluate the performance on both the training and validation set\n",
    "# YOUR_TURN??: predict_linear_classifier should be same as what you've implemented before, in the SVM section\n",
    "y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n",
    "train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).double().mean().item()\n",
    "print('training accuracy: %.2f%%' % train_acc)\n",
    "y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n",
    "val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).double().mean().item()\n",
    "print('validation accuracy: %.2f%%' % val_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuV0BZvzJirI"
   },
   "source": [
    "Now use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n",
    "\n",
    "To get full credit for the assignment, your best model found through cross-validation should achieve an accuracy above 0.37 on the validation set.\n",
    "\n",
    "(Our best model was above 39.8% -- did you beat us?)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "68lmNVj31ddu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688647150691,
     "user_tz": -540,
     "elapsed": 73852,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "fc8fdc55-9a31-4b72-ea8f-c24a7b0ecb46"
   },
   "source": [
    "import os\n",
    "import eecs598\n",
    "from linear_classifier import Softmax, softmax_get_search_params, test_one_param_set\n",
    "\n",
    "# YOUR_TURN: find the best learning_rates and regularization_strengths combination\n",
    "#            in 'softmax_get_search_params'\n",
    "learning_rates, regularization_strengths = softmax_get_search_params()\n",
    "num_models = len(learning_rates) * len(regularization_strengths)\n",
    "\n",
    "####\n",
    "# It is okay to comment out the following conditions when you are working on svm_get_search_params.\n",
    "# But, please do not forget to reset back to the original setting once you are done.\n",
    "if num_models > 25:\n",
    "  raise Exception(\"Please do not test/submit more than 25 items at once\")\n",
    "elif num_models < 5:\n",
    "  raise Exception(\"Please present at least 5 parameter sets in your final ipynb\")\n",
    "####\n",
    "\n",
    "\n",
    "i = 0\n",
    "# As before, store your cross-validation results in this dictionary.\n",
    "# The keys should be tuples of (learning_rate, regularization_strength) and\n",
    "# the values should be tuples (train_acc, val_acc)\n",
    "results = {}\n",
    "best_val = -1.0   # The highest validation accuracy that we have seen so far.\n",
    "best_softmax_model = None # The Softmax object that achieved the highest validation rate.\n",
    "num_iters = 2000 # number of iterations\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for reg in regularization_strengths:\n",
    "    i += 1\n",
    "    print('Training Softmax %d / %d with learning_rate=%e and reg=%e'\n",
    "          % (i, num_models, lr, reg))\n",
    "\n",
    "    eecs598.reset_seed(0)\n",
    "    cand_softmax_model, cand_train_acc, cand_val_acc = test_one_param_set(Softmax(), data_dict, lr, reg, num_iters)\n",
    "\n",
    "    if cand_val_acc > best_val:\n",
    "      best_val = cand_val_acc\n",
    "      best_softmax_model = cand_softmax_model # save the classifier\n",
    "    results[(lr, reg)] = (cand_train_acc, cand_val_acc)\n",
    "\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "  train_acc, val_acc = results[(lr, reg)]\n",
    "  print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "         lr, reg, train_acc, val_acc))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "# save the best model\n",
    "path = os.path.join(GOOGLE_DRIVE_PATH, 'softmax_best_model.pt')\n",
    "best_softmax_model.save(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efougAmNCFLo"
   },
   "source": [
    "Run the following to visualize your cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IVhRe3-DBjPr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688628581097,
     "user_tz": -540,
     "elapsed": 1137,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "b49de714-5c07-47a2-9eb1-05b4a98ca48d"
   },
   "source": [
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbOlUcv6J7MM"
   },
   "source": [
    "Them, evaluate the performance of your best model on test set. To get full credit for this assignment you should achieve a test-set accuracy above 0.36.\n",
    "\n",
    "(Our best was just around 39.9% -- did you beat us?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wxkVdB-1ddx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688647172003,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "464a9671-0d82-4cf6-db19-ddf7c3e40fb9"
   },
   "source": [
    "y_test_pred = best_softmax_model.predict(data_dict['X_test'])\n",
    "test_accuracy = torch.mean((data_dict['y_test'] == y_test_pred).double()) * 100\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Joo4RbeoKECC"
   },
   "source": [
    "Finally, let's visualize the learned weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XDfxI7mR1ddz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688647178932,
     "user_tz": -540,
     "elapsed": 1442,
     "user": {
      "displayName": "정화식",
      "userId": "07586292415870435156"
     }
    },
    "outputId": "37d7654c-6759-47d7-cd51-8794d484aff9"
   },
   "source": [
    "w = best_softmax_model.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(3, 32, 32, 10)\n",
    "w = w.transpose(0, 2).transpose(1, 0)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
